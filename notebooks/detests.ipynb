{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elural/miniconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/elural/miniconda3/envs/pytorch/lib/python3.12/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "#Neural net\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import peft\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#Other modules\n",
    "import tqdm\n",
    "import os\n",
    "from abc import ABC, abstractclassmethod\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1 = {\n",
    "    \"Approach\": [],\n",
    "    \"Fold\": [],\n",
    "    \"Data\":[],\n",
    "    \"f1\": [],\n",
    "    \"recall\": [],\n",
    "    \"precision\": [],\n",
    "    \"f1_0\": [],\n",
    "    \"recall_0\": [],\n",
    "    \"precision_0\": [],\n",
    "    \"f1_1\": [],\n",
    "    \"recall_1\": [],\n",
    "    \"precision_1\": [],\n",
    "    \"ce\": [],\n",
    "    \"Epoch\":[]\n",
    "}\n",
    "\n",
    "task_1_pd = pd.DataFrame(task_1)\n",
    "task_1_pd.to_csv(\"task1_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kf_splits(dataset, target_label, n_splits=3):\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    return skf.split(dataset.data[\"text\"], dataset.data[target_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(dataset, idx):\n",
    "    return torch.utils.data.Subset(dataset, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "data[\"text\"] = data.apply(lambda sample:preprocess_tweet(sample[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETESTSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, add_context=False):\n",
    "        self.data = data\n",
    "        self.context = add_context\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.loc[idx]\n",
    "\n",
    "        source = sample[\"source\"]\n",
    "        id = sample[\"id\"]\n",
    "        comment_id = sample[\"comment_id\"]\n",
    "\n",
    "        text = sample[\"text\"]\n",
    "        context = \"\"\n",
    "        if self.context:\n",
    "            if sample[\"level1\"] != \"0\":\n",
    "                context += \" - \"+ self.data[self.data[\"id\"] == sample[\"level1\"]][\"text\"].values[0]\n",
    "\n",
    "            if sample[\"level2\"] != \"0\":\n",
    "                context += \" - \" + self.data[self.data[\"comment_id\"] == sample[\"level2\"]][\"text\"].values[0]\n",
    "\n",
    "            if sample[\"level3\"] != \"0\": #comprobar que no es el mismoo que level2?\n",
    "                context += \" - \" + self.data[self.data[\"comment_id\"] == sample[\"level3\"]][\"text\"].values[0]\n",
    "\n",
    "        #context = context if context != \"\" else None\n",
    "\n",
    "        stereotype_hard = sample[\"stereotype\"]\n",
    "        stereotype_soft = sample[\"stereotype_soft\"]\n",
    "\n",
    "        stereotype_annotators = torch.tensor([\n",
    "            sample[\"stereotype_a1\"],\n",
    "            sample[\"stereotype_a2\"],\n",
    "            sample[\"stereotype_a3\"],\n",
    "        ])\n",
    "\n",
    "        implicit_hard = sample[\"implicit\"]\n",
    "        implicit_soft = sample[\"implicit_soft\"]\n",
    "        \n",
    "        implicit_annotators = torch.tensor([\n",
    "            sample[\"implicit_a1\"],\n",
    "            sample[\"implicit_a2\"],\n",
    "            sample[\"implicit_a3\"],\n",
    "        ])\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"idx\": idx,\n",
    "            \"source\": source,\n",
    "            \"id\": id,\n",
    "            \"comment_id\": comment_id,\n",
    "            \"text\": (text, context),\n",
    "            \"stereotype_hard\": stereotype_hard,\n",
    "            \"stereotype_soft\": stereotype_soft,\n",
    "            \"stereotype_annotators\": stereotype_annotators,\n",
    "            \"implicit_hard\": implicit_hard,\n",
    "            \"implicit_soft\": implicit_soft,\n",
    "            \"implicit_annotators\": implicit_annotators,\n",
    "        }\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "detest = DETESTSDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotatorHead(nn.Module):\n",
    "    \"\"\"This class represents the classification head to append at the end of our Transformer\"\"\"\n",
    "    def __init__(self, in_neurons, out_neurons):\n",
    "        super(AnnotatorHead, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_features=in_neurons, out_features=64),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.BatchNorm1d(64),\n",
    "                torch.nn.Linear(in_features=64, out_features=32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.BatchNorm1d(32),\n",
    "                torch.nn.Linear(in_features=32, out_features=out_neurons),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"This class is our Transformer model for classification\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        _transformer,\n",
    "        _tokenizer,\n",
    "        _lora_cfg,\n",
    "        num_annotators=1,\n",
    "        device=device,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.tokenizer = _tokenizer\n",
    "        self.base_transformer = peft.LoraModel(_transformer, _lora_cfg, \"default\")\n",
    "        #Get hidden size\n",
    "        self.hidden_size = self.base_transformer.config.to_dict().get(\"hidden_size\")\n",
    "        #Output layer of MLP\n",
    "        self.mlp_output_size = kwargs.get(\"output_neurons_mlp\", 2)\n",
    "        #All the MLP in output layer\n",
    "        self.annotators = nn.ModuleList(\n",
    "            [\n",
    "                AnnotatorHead(self.hidden_size, self.mlp_output_size)\n",
    "                for _ in range(num_annotators)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def get_num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        text, context = x[0], x[1]\n",
    "        #Tokenize text\n",
    "        x = self.tokenizer(\n",
    "            text, context, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        #Input to our transformer\n",
    "        x = self.base_transformer(**x).last_hidden_state[:, 0]\n",
    "        #Get the output of each annotator\n",
    "        y = [ann(x) for ann in self.annotators]\n",
    "        return torch.cat(y, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLossMultitask(nn.Module):\n",
    "    \"\"\" Custom Loss function for multi-task scenario\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predictions, target):\n",
    "        \n",
    "        #Each annotator produces an output for a sample. The loss for a sample is calculated wrt the loss of each annotator\n",
    "        return  torch.sum(self.bce(predictions, target), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predictions, target):\n",
    "        target = target.unsqueeze(dim=1)\n",
    "        target = torch.cat((1 - target, target), dim=1)\n",
    "        return self.ce(predictions, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractclassmethod\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ResultsParser:\n",
    "    def __init__(self, task) -> None:\n",
    "        self.task = task\n",
    "    \n",
    "    def parse(self, results):\n",
    "        res = {}\n",
    "        if self.task.startswith(\"stereotype\"):\n",
    "            #Class 0\n",
    "            res[\"precision_0\"] = results[\"0.0\"][\"precision\"]\n",
    "            res[\"recall_0\"] = results[\"0.0\"][\"recall\"]\n",
    "            res[\"f1_0\"] = results[\"0.0\"][\"f1-score\"]\n",
    "            #Class 1\n",
    "            res[\"precision_1\"] = results[\"1.0\"][\"precision\"]\n",
    "            res[\"recall_1\"] = results[\"1.0\"][\"recall\"]\n",
    "            res[\"f1_1\"] = results[\"1.0\"][\"f1-score\"]\n",
    "            #Macro\n",
    "            res[\"f1\"] = results[\"macro avg\"][\"f1-score\"]\n",
    "            res[\"precision\"] = results[\"macro avg\"][\"precision\"]\n",
    "            res[\"recall\"] = results[\"macro avg\"][\"recall\"]\n",
    "            #Cross Entropy\n",
    "            res[\"ce\"] = results[\"cross_entropy\"]\n",
    "        return res\n",
    "\n",
    "\n",
    "class ScoreEvaluator(ABC):\n",
    "    def __init__(self, task) -> None:\n",
    "        self.task = task\n",
    "        self.results_parser = ResultsParser(self.task)\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def get_precision_recall_f1(self, target, preds):\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def get_ce_score(self, target, preds):\n",
    "        pass\n",
    "\n",
    "    def get_results_task(self, target_hard, target_soft, preds):\n",
    "        res = {}\n",
    "        if self.task.startswith(\"stereotype\"):\n",
    "            f1_scores = self.get_precision_recall_f1(target_hard, preds)\n",
    "            ce_score = self.get_ce_score(target_soft, preds)\n",
    "            res = {\"cross_entropy\": ce_score, **f1_scores}\n",
    "        \n",
    "        return self.results_parser.parse(res)\n",
    "\n",
    "\n",
    "class HardScoreEvaluator(ScoreEvaluator):\n",
    "    def __init__(self, task) -> None:\n",
    "        super().__init__(task)\n",
    "        self.bce = torch.nn.BCELoss()\n",
    "\n",
    "    def get_precision_recall_f1(self, target, preds):\n",
    "        # Convert logits to probabilities and get the classes\n",
    "        preds = torch.sigmoid(preds) >= 0.5\n",
    "        # Compute F1-score\n",
    "        res = classification_report(target, preds, output_dict=True)\n",
    "        return res\n",
    "\n",
    "    def get_ce_score(self, target, preds):\n",
    "        # Logits -> Probabilities\n",
    "        preds = torch.sigmoid(preds)\n",
    "        # BCE\n",
    "        return self.bce(preds, target).item()\n",
    "\n",
    "\n",
    "class SoftScoreEvaluator(ScoreEvaluator):\n",
    "    def __init__(self, task):\n",
    "        super().__init__(task)\n",
    "        self.task = task\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_precision_recall_f1(self, target, preds):\n",
    "        # Logits -> Softmax probabilities -> Get most probable class\n",
    "        preds = torch.argmax(preds.softmax(dim=1), dim=1)\n",
    "        #F1-score\n",
    "        res = classification_report(target, preds, output_dict=True)\n",
    "        return res\n",
    "\n",
    "    def get_ce_score(self, target, preds):\n",
    "        #Add an extra dimension to targets\n",
    "        target = target.unsqueeze(dim=1)\n",
    "        #Probabilities for both classes\n",
    "        target = torch.cat((1 - target, target), dim=1)\n",
    "        #Cross entropy automatically applies softmax\n",
    "        return self.ce(preds, target).item()\n",
    "\n",
    "\n",
    "class MultiTaskScoreEvaluator(ScoreEvaluator):\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_precision_recall_f1(target, preds):\n",
    "        #From logits to  probs\n",
    "        preds = torch.sigmoid(preds)\n",
    "        #Get number of votes for each class\n",
    "        preds_annotator_votes_zero = (preds < 0.5).sum(dim=1)\n",
    "        preds_annotator_votes_ones = (preds >= 0.5).sum(dim=1)\n",
    "        #Majority voting\n",
    "        preds = torch.where(preds_annotator_votes_ones >= preds_annotator_votes_zero, 1, 0)\n",
    "        res = classification_report(target, preds, output_dict=True)\n",
    "        return res\n",
    "\n",
    "    def get_ce_score(self, target, preds):\n",
    "        #Add extra dimension to targets\n",
    "        target.unsqueeze_(dim=1)\n",
    "        #Concatenate outputs\n",
    "        target = torch.cat((1 - target, target), dim=1)\n",
    "        #Sigmoid to predictions\n",
    "        preds = torch.sigmoid(preds)\n",
    "        #Number of annotators for each class, add extra dimension\n",
    "        preds_annotator_votes_zero = (preds < 0.5).sum(dim=1).unsqueeze(dim=1)\n",
    "        preds_annotator_votes_ones = (preds >= 0.5).sum(dim=1).unsqueeze(dim=1)\n",
    "        #(B, 2)\n",
    "        preds = torch.cat((preds_annotator_votes_zero, preds_annotator_votes_ones), dim=1).float()\n",
    "        return self.ce(preds, target).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Recollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsRecollector:\n",
    "    def __init__(self, pd_data, csv_path) -> None:\n",
    "        self.pd_data = pd_data\n",
    "        self.csv_path =  csv_path\n",
    "    \n",
    "    def add_data_to_dataframe(self, dict_row):\n",
    "        self.pd_data = pd.concat([self.pd_data, pd.Series(dict_row).to_frame().T], ignore_index=True)\n",
    "    \n",
    "    def save_statistics(self):\n",
    "        self.pd_data.to_csv(self.csv_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        opt,\n",
    "        criterion,\n",
    "        dataset,\n",
    "        scorer,\n",
    "        statistic_rc,\n",
    "        accumulated_batch_size=8,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.criterion = criterion\n",
    "        self.dataset = dataset\n",
    "        self.accumulated_batch_size = (\n",
    "            accumulated_batch_size / 8\n",
    "        )  # My GPU only fits batch size of 8, so we are going to accumulate gradient\n",
    "        self.model_default_weights_path = \"model_default.pt\"\n",
    "        torch.save(self.model.state_dict(), self.model_default_weights_path)\n",
    "        self.statistic_rc = statistic_rc\n",
    "        self.scorer = scorer\n",
    "\n",
    "    def dev(self, dev_dataloader, task_name, task_label, epoch):\n",
    "        dev_loss = 0\n",
    "        target_preds_hard = torch.tensor([])\n",
    "        target_preds_soft = torch.tensor([])\n",
    "        dev_preds = torch.tensor([])\n",
    "        self.model.eval()\n",
    "\n",
    "        with tqdm.tqdm(\n",
    "            iter(dev_dataloader), desc=\"Dev epoch \" + str(epoch), unit=\"batch\"\n",
    "        ) as tepoch:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(tepoch):\n",
    "                    # Data\n",
    "                    text = batch[\"text\"]\n",
    "                    # Label\n",
    "                    task_hard_labels = batch[task_name + \"_hard\"].float()\n",
    "                    task_soft_labels = batch[task_name + \"_soft\"].float()\n",
    "                    # label to perform training\n",
    "                    task_target_labels = (\n",
    "                        batch[task_name + \"_\" + task_label].float().to(device)\n",
    "                    )\n",
    "                    \n",
    "                    # Forward\n",
    "                    preds = self.model(text).squeeze()\n",
    "                    # Compute loss and propagate bacckward\n",
    "                    loss = self.criterion(preds, task_target_labels)\n",
    "                    # Accumulate loss\n",
    "                    dev_loss += loss.mean()\n",
    "                    # Store outputs\n",
    "                    dev_preds = torch.cat((dev_preds, preds.cpu()))\n",
    "                    # Hard and soft labels\n",
    "                    target_preds_hard = torch.cat((target_preds_hard, task_hard_labels))\n",
    "                    target_preds_soft = torch.cat((target_preds_soft, task_soft_labels))\n",
    "                res = self.scorer.get_results_task(\n",
    "                    target_preds_hard, target_preds_soft, dev_preds\n",
    "                )\n",
    "\n",
    "                print(res)\n",
    "                print(f\"{dev_loss / len(dev_dataloader)}\")\n",
    "\n",
    "    def train(self, n_epochs, task_name, task_label):\n",
    "        kfold_splitter = get_kf_splits(self.dataset, task_name)\n",
    "\n",
    "        for fold, (train_idx, dev_idx) in enumerate(kfold_splitter):\n",
    "            self.model.load_state_dict(torch.load(self.model_default_weights_path))\n",
    "            print(f\"Fold {fold}\")\n",
    "            # DataLoader\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                create_subset(detest, train_idx),\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                num_workers=8,\n",
    "            )\n",
    "            dev_loader = torch.utils.data.DataLoader(\n",
    "                create_subset(detest, dev_idx),\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                num_workers=8,\n",
    "            )\n",
    "\n",
    "            train_best_f1_score = 0\n",
    "            train_best_ce_score = 1000\n",
    "            train_best_results = {}\n",
    "\n",
    "            dev_best_f1_score = 0\n",
    "            dev_best_ce_score = 1000\n",
    "            dev_best_results = {}\n",
    "            for epoch in range(n_epochs):\n",
    "                with tqdm.tqdm(\n",
    "                    iter(train_loader), desc=\"Train epoch \" + str(epoch), unit=\"batch\"\n",
    "                ) as tepoch:\n",
    "                    self.model.train()\n",
    "                    # Train Loop\n",
    "                    train_loss = 0\n",
    "                    train_preds = torch.tensor([], dtype=torch.float)\n",
    "                    target_preds_hard = torch.tensor([], dtype=torch.float)\n",
    "                    target_preds_soft = torch.tensor([], dtype=torch.float)\n",
    "\n",
    "                    for batch_idx, batch in enumerate(tepoch):\n",
    "                        # Data\n",
    "                        text = batch[\"text\"]\n",
    "                        # Label\n",
    "                        task_hard_labels = batch[task_name + \"_hard\"].float()\n",
    "                        task_soft_labels = batch[task_name + \"_soft\"].float()\n",
    "                        # label to perform training\n",
    "                        task_target_labels = (\n",
    "                            batch[task_name + \"_\" + task_label].float().to(device)\n",
    "                        )\n",
    "                        # Forward\n",
    "                        preds = self.model(text).squeeze()\n",
    "                        # Compute loss and propagate bacckward\n",
    "                        loss = self.criterion(preds, task_target_labels)\n",
    "                        # Propagate backward\n",
    "                        loss.mean().backward()\n",
    "                        # Accumulate loss\n",
    "                        train_loss += loss.mean()\n",
    "                        # Store outputs\n",
    "                        train_preds = torch.cat((train_preds, preds.cpu()))\n",
    "                        # Hard and soft labels\n",
    "                        target_preds_hard = torch.cat(\n",
    "                            (target_preds_hard, task_hard_labels)\n",
    "                        )\n",
    "                        target_preds_soft = torch.cat(\n",
    "                            (target_preds_soft, task_soft_labels)\n",
    "                        )\n",
    "                        # Update  gradients\n",
    "                        if (batch_idx + 1) % self.accumulated_batch_size == 0:\n",
    "                            self.opt.step()\n",
    "                            self.opt.zero_grad()\n",
    "                    results = self.scorer.get_results_task(\n",
    "                        target_preds_hard, target_preds_soft, train_preds\n",
    "                    )\n",
    "                    print(results)\n",
    "                    if (\n",
    "                        results[\"f1\"] >= train_best_f1_score\n",
    "                        and results[\"ce\"] <= train_best_ce_score\n",
    "                    ):\n",
    "                        train_best_f1_score = results[\"f1\"]\n",
    "                        train_best_ce_score = results[\"ce\"]\n",
    "                        train_best_results = {\n",
    "                            \"Approach\": task_name + \"_\" + task_label,\n",
    "                            \"Epoch\": epoch,\n",
    "                            \"Fold\": fold,\n",
    "                            \"Data\": \"train\",\n",
    "                            **results,\n",
    "                        }\n",
    "                    print(f\"TRAIN: {train_loss / len(train_loader)}\")\n",
    "                    # self.dev(dev_loader, task_name, task_label, epoch)\n",
    "\n",
    "                    dev_loss = 0\n",
    "                    target_preds_hard = torch.tensor([])\n",
    "                    target_preds_soft = torch.tensor([])\n",
    "                    dev_preds = torch.tensor([])\n",
    "                    self.model.eval()\n",
    "\n",
    "                    # Dev loop\n",
    "                    with tqdm.tqdm(\n",
    "                        iter(dev_loader), desc=\"Dev epoch \" + str(epoch), unit=\"batch\"\n",
    "                    ) as tepoch:\n",
    "                        with torch.no_grad():\n",
    "                            for batch_idx, batch in enumerate(tepoch):\n",
    "                                # Data\n",
    "                                text = batch[\"text\"]\n",
    "                                # Label\n",
    "                                task_hard_labels = batch[task_name + \"_hard\"].float()\n",
    "                                task_soft_labels = batch[task_name + \"_soft\"].float()\n",
    "                                # label to perform training\n",
    "                                task_target_labels = (\n",
    "                                    batch[task_name + \"_\" + task_label]\n",
    "                                    .float()\n",
    "                                    .to(device)\n",
    "                                )\n",
    "                                # Forward\n",
    "                                preds = self.model(text).squeeze()\n",
    "                                # Compute loss and propagate bacckward\n",
    "                                loss = self.criterion(preds, task_target_labels)\n",
    "                                # Accumulate loss\n",
    "                                dev_loss += loss.mean()\n",
    "                                # Store outputs\n",
    "                                dev_preds = torch.cat((dev_preds, preds.cpu()))\n",
    "                                # Hard and soft labels\n",
    "                                target_preds_hard = torch.cat(\n",
    "                                    (target_preds_hard, task_hard_labels)\n",
    "                                )\n",
    "                                target_preds_soft = torch.cat(\n",
    "                                    (target_preds_soft, task_soft_labels)\n",
    "                                )\n",
    "\n",
    "                            results = self.scorer.get_results_task(\n",
    "                                target_preds_hard, target_preds_soft, dev_preds\n",
    "                            )\n",
    "                            print(results)\n",
    "                            if (\n",
    "                                results[\"f1\"] >= dev_best_f1_score\n",
    "                                and results[\"ce\"] <= dev_best_ce_score\n",
    "                            ):\n",
    "                                dev_best_f1_score = results[\"f1\"]\n",
    "                                dev_best_ce_score = results[\"ce\"]\n",
    "                                \n",
    "                                dev_best_results = {\n",
    "                                    \"Approach\": task_name + \"_\" + task_label,\n",
    "                                    \"Epoch\": epoch,\n",
    "                                    \"Fold\": fold,\n",
    "                                    \"Data\": \"dev\",\n",
    "                                    **results,\n",
    "                                }\n",
    "                            print(f\"DEV: {dev_loss / len(dev_loader)}\")\n",
    "            self.statistic_rc.add_data_to_dataframe(train_best_results)\n",
    "            self.statistic_rc.add_data_to_dataframe(dev_best_results)\n",
    "        self.statistic_rc.save_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0: 100%|██████████| 826/826 [06:38<00:00,  2.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_0': 0.8219329214474845, 'recall_0': 0.7653585370865009, 'f1_0': 0.7926375146292158, 'precision_1': 0.44884169884169883, 'recall_1': 0.5354058721934369, 'f1_1': 0.488317143607246, 'f1': 0.6404773291182309, 'precision': 0.6353873101445917, 'recall': 0.650382204639969, 'ce': 0.591110348701477}\n",
      "TRAIN: 0.5909661650657654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev epoch 0: 100%|██████████| 413/413 [01:32<00:00,  4.44batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_0': 0.7867898699520877, 'recall_0': 0.9445357436318816, 'f1_0': 0.8584764749813294, 'precision_1': 0.6447368421052632, 'recall_1': 0.28225806451612906, 'f1_1': 0.3926282051282051, 'f1': 0.6255523400547672, 'precision': 0.7157633560286754, 'recall': 0.6133969040740054, 'ce': 0.558488667011261}\n",
      "DEV: 0.5585132241249084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train epoch 1: 100%|██████████| 826/826 [06:41<00:00,  2.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_0': 0.8702209005947323, 'recall_0': 0.8417916581056092, 'f1_0': 0.8557702349869452, 'precision_1': 0.5938818565400844, 'recall_1': 0.6482440990213011, 'f1_1': 0.6198733828791633, 'f1': 0.7378218089330542, 'precision': 0.7320513785674083, 'recall': 0.7450178785634551, 'ce': 0.5126221179962158}\n",
      "TRAIN: 0.5124653577804565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev epoch 1: 100%|██████████| 413/413 [01:31<00:00,  4.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_0': 0.8260200153964589, 'recall_0': 0.8816762530813476, 'f1_0': 0.8529411764705882, 'precision_1': 0.5909090909090909, 'recall_1': 0.4792626728110599, 'f1_1': 0.5292620865139949, 'f1': 0.6911016314922915, 'precision': 0.7084645531527749, 'recall': 0.6804694629462037, 'ce': 0.5486724376678467}\n",
      "DEV: 0.5487388968467712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train epoch 2: 100%|██████████| 826/826 [06:37<00:00,  2.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_0': 0.8812860676009893, 'recall_0': 0.878569960961578, 'f1_0': 0.8799259183043523, 'precision_1': 0.6626712328767124, 'recall_1': 0.6683937823834197, 'f1_1': 0.6655202063628547, 'f1': 0.7727230623336034, 'precision': 0.7719786502388508, 'recall': 0.7734818716724989, 'ce': 0.4701634645462036}\n",
      "TRAIN: 0.47017231583595276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev epoch 2: 100%|██████████| 413/413 [01:48<00:00,  3.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_0': 0.8093205265030239, 'recall_0': 0.9346754313886606, 'f1_0': 0.8674928503336511, 'precision_1': 0.6761710794297352, 'recall_1': 0.3824884792626728, 'f1_1': 0.48859455481972036, 'f1': 0.6780437025766857, 'precision': 0.7427458029663796, 'recall': 0.6585819553256667, 'ce': 0.539480984210968}\n",
      "DEV: 0.5394885540008545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train epoch 3:   2%|▏         | 16/826 [02:00<1:41:28,  7.52s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m st_rc \u001b[38;5;241m=\u001b[39m StatisticsRecollector(pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask1_res.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask1_res.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, opt, criterion, detest, scorer, st_rc)\n\u001b[0;32m---> 15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstereotype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 121\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs, task_name, task_label)\u001b[0m\n\u001b[1;32m    119\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Store outputs\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((train_preds, preds\u001b[38;5;241m.\u001b[39mcpu()))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Hard and soft labels\u001b[39;00m\n\u001b[1;32m    123\u001b[0m target_preds_hard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    124\u001b[0m     (target_preds_hard, task_hard_labels)\n\u001b[1;32m    125\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "transformer = transformers.AutoModel.from_pretrained(model_name)\n",
    "lora_config = peft.LoraConfig(r=16, lora_alpha=16, lora_dropout=0.3)\n",
    "\n",
    "model = TransformerModel(\n",
    "    transformer, tokenizer, lora_config, num_annotators=1, **{\"output_neurons_mlp\": 2}\n",
    ")\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion = CEWithLogitsLoss()\n",
    "scorer = SoftScoreEvaluator(\"stereotype_soft\")\n",
    "st_rc = StatisticsRecollector(pd.read_csv('task1_res.csv'), 'task1_res.csv')\n",
    "trainer = Trainer(model, opt, criterion, detest, scorer, st_rc)\n",
    "trainer.train(5, \"stereotype\", \"soft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
